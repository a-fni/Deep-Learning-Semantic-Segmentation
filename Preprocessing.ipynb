{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ANOl2dvHhPB-",
        "Q86EWRt2sA5s",
        "Qu-s7Ez7E4wt"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup and libray inclusion"
      ],
      "metadata": {
        "id": "ANOl2dvHhPB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fjzES_PFgmv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e731c614-1ca1-4ab5-c1d6-e33ffcb0ad3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/Colab/AN2DL/Homework2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/Colab/AN2DL/Homework2\n",
        "\n",
        "# Installing Keras-CV\n",
        "!pip install -q --upgrade keras-cv albumentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inclusing libraries\n",
        "from google.colab                           import runtime\n",
        "from keras_cv.layers                        import AugMix, RandAugment, \\\n",
        "                                                    RandomHue, RandomSaturation\n",
        "from matplotlib.colors                      import ListedColormap\n",
        "from tensorflow                             import keras as tfk\n",
        "from tensorflow.keras                       import layers as tfkl\n",
        "from tensorflow.keras.applications          import ConvNeXtBase\n",
        "from tensorflow.keras.models                import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image   import ImageDataGenerator\n",
        "from tensorflow.keras.utils                 import to_categorical, Sequence\n",
        "from sklearn.metrics                        import accuracy_score, precision_score,\\\n",
        "                                                    recall_score, f1_score,\\\n",
        "                                                    confusion_matrix\n",
        "from sklearn.model_selection                import train_test_split\n",
        "from sklearn.utils                          import class_weight\n",
        "\n",
        "import albumentations                       as A\n",
        "import matplotlib.pyplot                    as plt\n",
        "import numpy                                as np\n",
        "import os\n",
        "import pandas                               as pd\n",
        "import seaborn                              as sns\n",
        "import tensorflow                           as tf"
      ],
      "metadata": {
        "id": "BM94QSaKrdpu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting seed for our execution environment."
      ],
      "metadata": {
        "id": "voHgQ2m7sCLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed: int = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "Gahno63wr4sF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a series of useful functions which will be called throught the notebook."
      ],
      "metadata": {
        "id": "0lvqLFRls6L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayImage(image, title=\"\") -> None:\n",
        "    \"\"\"\n",
        "    Function in charge of displaying an image\n",
        "\n",
        "    -------\n",
        "    image: image to be displayed\n",
        "    title: title of the image\n",
        "    \"\"\"\n",
        "\n",
        "    plt.imshow(image, cmap=\"grey\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def displaySegmentedImage(image_with_segmentation, title=\"Original Image\") -> None:\n",
        "    \"\"\"\n",
        "    Function in charge of displaying a segmented image\n",
        "\n",
        "    -------\n",
        "    image_with_segmentation: image to be displayed (NOT one-hot-encoded)\n",
        "    title: title of the image\n",
        "    \"\"\"\n",
        "\n",
        "    if image_with_segmentation.shape[0] != 2:\n",
        "        raise ValueError(\"Input array must have two components along axis 0 (image and segmentation).\")\n",
        "\n",
        "    # Extract image and segmentation\n",
        "    image   = image_with_segmentation[0]  # Original image\n",
        "    seg     = image_with_segmentation[1]  # Segmentation classes\n",
        "\n",
        "    # Define a colormap for segmentation classes\n",
        "    colours = ['black', 'brown', 'blue', 'yellow', 'orange']\n",
        "    cmap    = []\n",
        "    for label in range(5):\n",
        "        if np.any(seg == label):\n",
        "            cmap.append(colours[label])\n",
        "    cmap    = ListedColormap(cmap)\n",
        "\n",
        "    # Plot the results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Display the original image\n",
        "    axes[0].imshow(image, cmap='gray')\n",
        "    axes[0].set_title(title)\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Overlay segmentation on top of the original image\n",
        "    axes[1].imshow(image, cmap='gray')\n",
        "    axes[1].imshow(seg, cmap=cmap, alpha=0.5)  # Alpha for transparency\n",
        "    axes[1].set_title(\"Segmented Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Add colour legend\n",
        "    legend_elements = [\"background\", \"soil\", \"bedrock\", \"sand\", \"big rock\"]\n",
        "    legend_handles  = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colours[i], markersize=10) for i in range(5)]\n",
        "    axes[0].legend(legend_handles, legend_elements, title=\"Classes\", loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def toCategorical(mask, img_shape, num_classes):\n",
        "    mask_categorical = np.zeros((mask.shape[0], *img_shape, num_classes))\n",
        "    for i in range(mask.shape[0]):\n",
        "        mask_categorical[i] = to_categorical(mask[i], num_classes=num_classes)\n",
        "    return mask_categorical\n",
        "\n",
        "\n",
        "def goodnight() -> None:\n",
        "    \"\"\"\n",
        "    Function in charge of disconnecting current runtime\n",
        "    \"\"\"\n",
        "    runtime.unassign()"
      ],
      "metadata": {
        "id": "w579R6bss5jf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset loading and cleaning"
      ],
      "metadata": {
        "id": "Q86EWRt2sA5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains several images with an alien superimposed on them. We therefore need to perform some cleaning before applying general preprocessing. Also, since class-0 (*background*) does not count during evaluation, images which ONLY contain background serve no use in training, hence we can remove them as well."
      ],
      "metadata": {
        "id": "06VuJpc61HxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data    = np.load('mars.npz')\n",
        "images  = data['training_set']\n",
        "\n",
        "# Flag to signal that dataset hasn't been cleaned so far\n",
        "cleaned: bool = False"
      ],
      "metadata": {
        "id": "8tbV3ViTsXFi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dataset has been loaded, we actually clean it from irrelevant and troublesome samples."
      ],
      "metadata": {
        "id": "utb_6pDp0CwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index of a known outlier\n",
        "track_index: int = 669\n",
        "\n",
        "# List of indices of outlier samples\n",
        "outlier_indices: list[int] = []\n",
        "only_bg_indices: list[int] = []\n",
        "\n",
        "# Remove all images which have segmentation map equal to that of image at track_index == 669\n",
        "if not cleaned:\n",
        "    for i, image in enumerate(images):\n",
        "        if np.array_equal(image[1], images[track_index][1]):\n",
        "            outlier_indices.append(i)\n",
        "print(f\"{len(images)} ==[OUTLIERS]==> \", end=\"\")\n",
        "images = np.delete(images, outlier_indices, axis=0)\n",
        "print(f\"{len(images)} ==[ONLY-BG]==> \", end=\"\")\n",
        "\n",
        "# Remove all only-background images\n",
        "for i, image in enumerate(images):\n",
        "    if np.array_equal(image[1], np.zeros_like(image[1])):\n",
        "        only_bg_indices.append(i)\n",
        "images = np.delete(images, only_bg_indices, axis=0)\n",
        "print(f\"{len(images)}\")\n",
        "print(f\"Removed {len(outlier_indices)} outliers and {len(only_bg_indices)} only-background images\")\n",
        "\n",
        "# Updating flag so as to avoid re-cleaning\n",
        "cleaned: bool = True"
      ],
      "metadata": {
        "id": "DqPV1WwYxiEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc4d68b-fb83-4000-f12a-156acea0ca60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2615 ==[OUTLIERS]==> 2505 ==[ONLY-BG]==> 2498\n",
            "Removed 110 outliers and 7 only-background images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define custom weights for our segmentation classes here:"
      ],
      "metadata": {
        "id": "tNr3uVmoP5Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights: dict = {\n",
        "    0: 0,\n",
        "    1: 1,\n",
        "    2: 1,\n",
        "    3: 1,\n",
        "    4: 2,\n",
        "}\n",
        "\n",
        "# Normalise so as to have sum one of weights\n",
        "norm = sum(weights.values())\n",
        "for i in range(5):\n",
        "    weights[i] /= norm\n",
        "print(weights, sum(weights.values()))\n",
        "weights = list(weights.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a6s8TnP5fr",
        "outputId": "4c607f4b-d955-495e-fe3b-4f3460d37084"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.0, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.4} 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocessing"
      ],
      "metadata": {
        "id": "O6Bgcoyd2yj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having performed initial cleaning of the training set, we perform additional pre-processing to it. This will mainly consist in augmenting images, which will allow us to increase the number of training samples as well as improve generalisation of our model. Our first operation will normalise the whole set into the [0, 1] range and then divide it into training and validation sets."
      ],
      "metadata": {
        "id": "Jbru9RfS1EbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise dataset\n",
        "normalised_images = images.copy()\n",
        "normalised_images[:, 0, :, :] = (normalised_images[:, 0, :, :] / 255.0).astype(np.float32)\n",
        "\n",
        "# Divide dataset into training and validation splits\n",
        "train, validation = train_test_split(\n",
        "    normalised_images, test_size=0.3, random_state=seed\n",
        ")\n",
        "\n",
        "# Setting up some useful parameters for later\n",
        "base_shape  = train[0].shape        # Shape of sample (C * W * H)\n",
        "img_shape   = train[0][0].shape     # Shape of image (W * H)\n",
        "height      = img_shape[0]          # Height of image\n",
        "width       = img_shape[1]          # Width of image\n",
        "NUM_CLASSES = 5                     # Number of classes\n",
        "batch_size  = 64                    # Tensor flow dataset batch size\n",
        "aug_factor  = 10                    # Number of augmentations for each sample\n",
        "\n",
        "# Logging\n",
        "print(f\"Training set shape:   {train.shape}\")\n",
        "print(f\"Validation set shape: {validation.shape}\")"
      ],
      "metadata": {
        "id": "AE7fG4muGAUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfc2e01-22b3-40b1-d038-c70db5f5045f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape:   (1748, 2, 64, 128)\n",
            "Validation set shape: (750, 2, 64, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define our custom augmentation pipeline which will allow us to distort our dataset without losing mask information for segmentation."
      ],
      "metadata": {
        "id": "GyxUGyRcq2Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAugmentationPipeline(height=height, width=width):\n",
        "    return A.Compose(\n",
        "        [\n",
        "            # Geometric transformations\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # Small rotations (±30°) and scaling/translation/shear within a reasonable range\n",
        "            A.Affine(\n",
        "                scale=(0.9, 1.1),\n",
        "                rotate=(-30, 30),\n",
        "                shear=(-10, 10),\n",
        "                translate_percent=(0.1, 0.1),\n",
        "                interpolation=1,      # For image, nearest-neighbor is acceptable since grayscale. If you prefer smoother interpolation for the image, use interpolation=2 or similar.\n",
        "                mask_interpolation=0, # Ensure mask uses nearest-neighbor\n",
        "                p=0.5\n",
        "            ),\n",
        "            # Random crop and then resize back to original dimension\n",
        "            A.RandomCrop(height=int(0.8*height), width=int(0.8*width), p=0.5),\n",
        "            A.Resize(height=height, width=width, interpolation=1, p=1.0),  # always resize back\n",
        "\n",
        "            # Intensity-based augmentations (grayscale-friendly)\n",
        "            A.RandomBrightnessContrast(\n",
        "                brightness_limit=0.2,\n",
        "                contrast_limit=0.2,\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.5),\n",
        "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "\n",
        "            # Mild blur/sharpen\n",
        "            A.GaussianBlur(blur_limit=(3,3), p=0.3),\n",
        "\n",
        "            # Mild elastic deformation\n",
        "            A.ElasticTransform(\n",
        "                alpha=1,\n",
        "                sigma=50,\n",
        "                interpolation=1,  # nearest-neighbor to avoid label artifacts\n",
        "                border_mode=4,    # reflect border\n",
        "                p=0.2\n",
        "            ),\n",
        "        ],\n",
        "        additional_targets={'mask': 'mask'}\n",
        "    )"
      ],
      "metadata": {
        "id": "mMPNCff8Phc8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will perform augmentations by creating and executing the custom pipeline on each sample. We augment `aug_factor` times each sample and store the original sample as well as all its augmentations in our dataset."
      ],
      "metadata": {
        "id": "dvsy5vpEq_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom augmentation factor based on class frequencies\n",
        "class_count: list[int] = [0 for _ in range(NUM_CLASSES)]\n",
        "for image in train:\n",
        "    for i in range(NUM_CLASSES):\n",
        "        class_count[i] += np.sum(image[1] == i)\n",
        "\n",
        "# Convert to np.array, obtain frequencies and invert\n",
        "class_count         = np.array(class_count)\n",
        "class_frequencies   = class_count / (width * height * len(train))\n",
        "inverse_frequencies = np.ceil(1 / class_frequencies)\n",
        "\n",
        "# Limit augmentation factors to sum of other inverse_frequencies\n",
        "aug_factors: list[int] = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    sum_others = 0\n",
        "    for j in range(NUM_CLASSES):\n",
        "        if i != j:\n",
        "            sum_others += inverse_frequencies[j]\n",
        "    aug_factors.append(int(min(inverse_frequencies[i], sum_others)))\n",
        "aug_factors = np.array(aug_factors)\n",
        "print(aug_factors)"
      ],
      "metadata": {
        "id": "VWI2MPlqxOoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d096a2ee-61c3-451f-b948-454f490544b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5  3  5  6 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform augmentations and export\n",
        "def augmentImagesWithMask(images, aug_factors=aug_factors):\n",
        "    \"\"\"\n",
        "    Augment and export the dataset to a single .npz file.\n",
        "    Args:\n",
        "        images (numpy array): Original images of shape (num_samples, 2, height, width).\n",
        "        aug_factor (int): Number of augmentations per sample.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain an augmentation pipeline and create augmentation placeholders\n",
        "    augmentation_pipeline = getAugmentationPipeline()\n",
        "    augmented_images = []\n",
        "    augmented_masks  = []\n",
        "\n",
        "    # Augment each sample\n",
        "    for image_with_mask in images:\n",
        "        image = image_with_mask[0].astype(np.float32)   # Ensure correct data type\n",
        "        mask  = image_with_mask[1].astype(np.uint8)     # Ensure correct data type\n",
        "\n",
        "        # Add original samples as well\n",
        "        augmented_images.append(image)\n",
        "        augmented_masks.append(mask)\n",
        "\n",
        "        # Obtain all pixel classes of the image\n",
        "        unique_classes = np.unique(mask)\n",
        "\n",
        "        # Augment as many times as the maximum ceil(aug_factors) of the present classes\n",
        "        aug_factor = np.max(aug_factors[unique_classes])\n",
        "\n",
        "        # Generate augmentations\n",
        "        for _ in range(aug_factor):\n",
        "            augmented = augmentation_pipeline(image=image, mask=mask)\n",
        "\n",
        "            augmented_image = augmented[\"image\"].astype(np.float32)   # Explicit conversion\n",
        "            augmented_mask  = augmented[\"mask\"].astype(np.uint8)      # Explicit conversion\n",
        "\n",
        "            augmented_images.append(augmented_image)\n",
        "            augmented_masks.append(augmented_mask)\n",
        "\n",
        "    # Convert to numpy arrays and stack together\n",
        "    augmented_images = np.array(augmented_images, dtype=np.float32)\n",
        "    augmented_masks  = np.array(augmented_masks, dtype=np.uint8)\n",
        "    augmented        = np.stack((augmented_images, augmented_masks), axis=1)\n",
        "\n",
        "    # Return stacked dataset\n",
        "    return augmented"
      ],
      "metadata": {
        "id": "fMhg59naieoA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now augment and complete preprocessing by converting all masks into one-hot encoding, which is necessary for training."
      ],
      "metadata": {
        "id": "y5FY9Yg_rNm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform augmentations and save as npz\n",
        "print(f\"TRAINING: {train.shape} ==[AUGMENTATION]=>\", end=\" \")\n",
        "augmented_train = augmentImagesWithMask(train)\n",
        "print(f\"{augmented_train.shape} ==[CATEGORICAL]=>\", end=\" \")\n",
        "\n",
        "# Convert to categorical\n",
        "augmented_train_images  = augmented_train[:, 0]\n",
        "augmented_train_masks   = toCategorical(augmented_train[:, 1], img_shape, NUM_CLASSES)\n",
        "print(f\"{augmented_train_images.shape} + {augmented_train_masks.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERnaqD3RjY20",
        "outputId": "f1932620-4cfb-4b19-c764-cde7447776b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING: (1748, 2, 64, 128) ==[AUGMENTATION]=> (11345, 2, 64, 128) ==[CATEGORICAL]=> (11345, 64, 128) + (11345, 64, 128, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting also validation to categorical."
      ],
      "metadata": {
        "id": "D_kIbgLbti1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"VALIDATION: {validation.shape} ==[CATEGORICAL]=> \", end=\"\")\n",
        "validation_images       = validation[:, 0].astype(np.float32)\n",
        "validation_segs         = toCategorical(validation[:, 1], img_shape, NUM_CLASSES).astype(np.int8)\n",
        "print(f\"{validation_images.shape} + {validation_segs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4j25V7rsnv8",
        "outputId": "b6b2dac7-977e-48df-fed0-1df6ece776eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION: (750, 2, 64, 128) ==[CATEGORICAL]=> (750, 64, 128) + (750, 64, 128, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we export the augmented and preprocessed training and validation sets."
      ],
      "metadata": {
        "id": "TJxs37mJrU2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez(\n",
        "    \"mars_augmented.npz\",\n",
        "    train_images=augmented_train_images,\n",
        "    train_segs=augmented_train_masks,\n",
        "    val_images=validation_images,\n",
        "    val_segs=validation_segs,\n",
        "    weights=weights,\n",
        ")\n",
        "print(\"== EXPORTED ==\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqhKH88pL37",
        "outputId": "b8333192-91d1-4046-c326-634d85f1af3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== EXPORTED ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnecting"
      ],
      "metadata": {
        "id": "Qu-s7Ez7E4wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disconnect runtime to avoid consuming resources if the notebook has completed its execution."
      ],
      "metadata": {
        "id": "4cai9Eqb0318"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goodnight()"
      ],
      "metadata": {
        "id": "nXc5vMrzsouz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}